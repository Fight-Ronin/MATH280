\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage{environ}

\newcommand{\inv}{^{-1}}

\newcounter{pnum}
\NewEnviron{problem}{
    \stepcounter{pnum}
    \begin{center}
        \fbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{\thepnum.} \BODY
        \end{minipage}}
    \end{center}

    \textbf{Solution. }
}

\title{\vspace{-4em}Homework 7 (differential equations)}
\author{Hanzhang Yin}
\begin{document}

\maketitle

Unless otherwise specified, you may assume that any differential equations defined below are sufficiently nice that the existence and uniqueness theorems apply.



\begin{problem}
    In this problem, we'll use homotopy continuation to give a proof of the quadratic formula. It may be helpful to use the homotopy continuation notebook to plot the problem.

    \hspace{2em} Let \(f(x) = x^2 + ax + b\) a quadratic function where \(a,b\) are real numbers such that \(a^2>4b\) and \(a\neq 0\). Let \(g(x) = f(x) -f(0) = x^2 + ax\).
    \begin{enumerate}[a)]
        \item Find the two real roots of \(g\).
        \item Determine the standard homotopy \(H(x,t)\).
        \item Use the homotopy to write down the standard homotopy differential equation
        \[x'(t) = - \frac{H_t}{H_x}.\]
        \item Make the change of variable \(y=2x+a\) and solve the resulting differential equation.
        Don't forget the initial condition \(y(0)=2x(0) + a\), where \(x(0)\) is one of the two roots from (a).
        \item Determine the paths \(x(t)\) and the values \(x(1)\), which should match the usual quadratic formula.
        \item Explain the paths in terms of the quadratic formula. Point out what this proof and the standard proof of the quadratic formula have in common.
    \end{enumerate}
\end{problem}

\subsubsection*{(a)}
\begin{proof}
    Given $g(x) = x^2 + ax$, set $g(x) = 0$.
    \[ \Rightarrow x^2 + ax = 0 \Rightarrow x(x + a) = 0 \]
    Therefore, the two real roots are $x = 0, -a$. 
\end{proof}

\subsubsection*{(b)}
\begin{proof}
    The standard homotopy transition from $g(x)$ to $f(x)$ as $t$ goes from 0 to 1. Since $f(x) = g(x) + b$, the homotopy is:
    \[ H(x,t) = x^2 + ax + tb \]
    at $t = 0$:
    \[ H(x,0) = x^2 + ax = g(x) \] 
    at $t = 1$:
    \[ H(x,1) = x^2 + ax + b = f(x) \] 
\end{proof}

\subsubsection*{(c)}
\begin{proof}
    First, compute the partial derivatives:
    \[
        H_x = \frac{\partial H}{\partial x} = 2x + a, \quad H_t = \frac{\partial H}{\partial t} = b.
    \]
    The standard homotopy differential equation is:
    \[
        x'(t) = -\frac{H_t}{H_x} = -\frac{b}{2x + a}.
    \]
\end{proof}

\subsubsection*{(d)}
\begin{proof}
    With $y = 2x + a$, we have:
    \[
        \frac{dy}{dt} = 2x'(t)
    \]
    Subsitute $x'(t)$ from part $(c)$:
    \[ 
        \frac{dy}{dt} = 2\left( -\frac{b}{2x + b} \right) = -\frac{2b}{y} 
        \Rightarrow y dy = -2b dt 
        \Rightarrow \int y dy = \int -2b dt \Longrightarrow \frac{1}{2}y^2 = 2bt + C
        \Rightarrow y^2 = -4bt + C
    \]
    Apply the initial condition, If we choose $x(0) = 0$, then:
    \[ y(0) = 2x(0) + a = a \]
    Sub $t = 0$ and $y(0) = a$ into the equation:
    \[ a^2 = -4b \cdot 0 + C \Longrightarrow a^2 \]
    Hence, 
    \[ y^2 = a^2 + 4bt \]
\end{proof}

\subsubsection*{(e)}
\begin{proof}
    From the expression $y^2 = a^2 - 4bt$, we solve for $y$:
    \[ \Rightarrow y = \pm \sqrt{a^2 - 4bt} \]
    Recall $y = 2x + a$, so:
    \[ 2x + a = \pm \sqrt{a^2 - 4bt} \Rightarrow x(t) = \frac{-a \pm \sqrt{a^2 - 4bt}}{2} \]
    At $t = 1$:
    \[ x(1) = \frac{-a \pm \sqrt{a^2 - 4b}}{2} \]
    This is exactly the quadratic formula for the roots for $f(x) = x^2 + ax + b$
\end{proof}

\subsubsection*{(f)}
The path $x(t)$ represent the continuous transformation of the roots from the simpler quadratic $g(x)$ to the original quadratic $f(x)$ as $t$ increases from $0$ to $1$.
Each path follows the equation:
\[ x(t) = \frac{-a \pm \sqrt{a^2 - 4bt}}{2} \]
At $t = 0$, the roots are those of $g(x)$, and at $t=1$, they are roots of $f(x)$ given quadrat5ic formula,. This homoptopy-based proof indicates the standard proof in that both derive the quadratic formula by manipulating the equation to isolate $x$.
The homotopy method provides a dynamic point of view, showing hot the roots evolve compare to standard ``static'' proof.


\begin{problem}
    Suppose \(f\) is differentiable on \((a,b)\). Show that the mean value theorem implies the fundamental theorem of the derivative:\\
    
    MVT: given \(x,y\) in \((a,b)\) there is some \(z\) in \((a,b)\) such that
    \[\frac{f(x) - f(y)}{x-y} = f'(z).\]

    FTD: if \(f' = 0\) on \((a,b)\) then \(f\) is constant.\\

    Observe that the FTD is a simple example of the uniqueness theorem for differential equations.
\end{problem}


\begin{proof}
    Let $f$ be a differentiable function on $(a,b)$ such that $f'(x) = 0$, $\forall x \in (a,b)$.
    \\
    For any two point $x,y \in (a,b)$, where $x \neq y$, the MVT states that there exists some $z$ between $x$ and $y$ (i.e. $z \in (a,b)$). Also noting that $f'(z) = 0$ for all $z \in (a, b)$, we have:
    \[ f'(z) = \frac{f(x) - f(y)}{x - y} = 0 \]
    \[ \Rightarrow f(x) - f(y) = 0 \Rightarrow f(x) = f(y) \]
    In this case, since the value of $f$ does not depend of choice of $x \in (a, b)$, $f$ must be a constant function on $(a,b)$.
\end{proof}


\begin{problem}
    Define a mystery function \(f\) by the initial value problem:
    \[f'' = f\hspace{4em}f(0) = 0\ ,\ f'(0) = 1,\]
    and \(g=f'\). From this definition, verify the following identity:
    \[f(x+y) = f(x)g(y) + f(y)g(x)\]
    without explicitly determining \(f\) and \(g\).
    
    \hspace{2em}This is the pythagorean identity for the hyperbolic trig functions. The solution should have \emph{no} geometry (unless you want to develop it from first principles).
\end{problem}

\begin{proof}
    Let 
    \[ H(x,y) = f(x + y) - [f(x)g(y) + f(y)g(x)] \]
    We aim to show that $H(x,y) = 0$ for all $x,y$.
    First, lets differentiate $H$ w.r.t. $x$:
    \[
        \frac{\partial H}{\partial x} = f'(x + y) - \left[f'(x)g(y) + f(y)g'(x)\right] = g(x + y) - \left[g(x)g(y) + f(y)f(x)\right].
    \]
    Differentiate again with respect to \( x \):
    \[
        \frac{\partial^2 H}{\partial x^2} = g'(x + y) - \left[g'(x)g(y) + g(x)g'(y) + f(y)f''(x) + f'(y)f'(x)\right].
    \]
    Using \( g' = f \) and \( f'' = g \), this simplifies to:
    \[
        \frac{\partial^2 H}{\partial x^2} = f(x + y) - 2\left[f(x)g(y) + f(y)g(x)\right].
    \]
    Substituting the definition of \( H(x, y) \):
    \[
        \frac{\partial^2 H}{\partial x^2} = H(x, y) - [f(x)g(y) + f(y)g(x)].
    \]
    Thus:
    \[
        \frac{\partial^2 H}{\partial x^2} - H(x, y) = -[f(x)g(y) + f(y)g(x)].
    \]
    Given that $H(x,y) = f(x + y) - [f(x)g(y) + f(y)g(x)]$, subsitute it back leads to:
    \[ 
        \frac{\partial^2 H}{\partial x^2} - H(x, y) = -H(x,y)
        \Rightarrow
        \frac{\partial^2 H}{\partial x^2} = 0.
    \]
    The general solution to \( \frac{\partial^2 H}{\partial x^2} = 0 \) is:
    \[
        H(x, y) = A(y) x + B(y),
    \]
    where \( A(y) \) and \( B(y) \) are functions of \( y \) only.
    \\
    Now, we need to evaluate \( H(x, y) \) and its first derivative at \( x = 0 \):
    \begin{enumerate}
        \item At \( x = 0 \):
        \[
            H(0, y) = f(y) - [f(0)g(y) + f(y)g(0)] = f(y) - [0 \cdot g(y) + f(y) \cdot 1] = f(y) - f(y) = 0.
        \]
        Thus:
        \[
            B(y) = 0.
        \]
        \item First Derivative at \( x = 0 \):
        \[
            \left.\frac{\partial H}{\partial x}\right|_{x=0} = g(y) - [g(0)g(y) + f(y)f(0)] = g(y) - [1 \cdot g(y) + f(y) \cdot 0] = g(y) - g(y) = 0.
        \]
        From the general solution:
        \[
            \left.\frac{\partial H}{\partial x}\right|_{x=0} = A(y).
        \]
        Thus:
        \[
            A(y) = 0.
        \]
    \end{enumerate}
    With \( A(y) = 0 \) and \( B(y) = 0 \), the general solution simplifies to:
    \[
        H(x, y) = 0 \quad \text{for all } x, y.
    \]
    Therefore:
    \[
        f(x + y) = f(x)g(y) + f(y)g(x).
    \]
\end{proof}


\begin{problem}
    Derivatives aren't very different for complex numbers. In particular, it's still true that
    \[\frac{d}{dx} e^{rx} = re^{rx}\]
    when \(r\) is a complex number. The usual differential equation \(f'=f, f(0)=1\) still defines \(e^x\) and the existence and uniqueness theorems apply in the complex setting. 
    
    \hspace{2em}Other exponent identities like \((e^x)^r = e^{rx}\) and \(e^{x+a} = e^xe^a\) also work (can be established with the same proofs, using existence and uniqueness, if you want).
    \begin{enumerate}[a)]
        \item Verify the following identities
            \[\sin(x) = \frac{e^{ix} - e^{-ix}}{2i},\]
            \[\cos(x) = \frac{e^{ix} + e^{-ix}}{2}.\]
        \item Combine those identities into
            \[e^{ix} = \cos(x) + i\sin(x).\]
        \item Use that to establish De Moivre's formula,
            \[(\cos(x) + i\sin(x))^n = \cos(nx) + i\sin(nx).\]
        \item Take \(n=2\) in De Moivre's formula and observe that it furnishes a quick simultaneous proof of the double-angle identities for \(\sin\) and \(\cos\)  (and an easy way to remember them). Remark: in spite of what you may have heard in other courses, calculators don't use power series to evaluate trig functions, but rather (more or less) from a version of this identity.
    \end{enumerate}
\end{problem}

\subsubsection*{(a)}
\begin{proof}
    Noting that $f = sin(x)$ has the unique solution to $f'' = -f$, $f(0) = 0$, $f'(0) = 1$. In this case we want to check whether the 
    performance of $g = \frac{e^{ix} - e^{-ix}}{2i}$ matches.
    \[ g' = \frac{1}{2}e^{-ix}(1 + e^{2ix}) \Rightarrow g'(0) = \frac{1}{2} + \frac{1}{2} = 1 \]
    \[ g'' = \frac{1}{2} ie^{-ix}(-1 + e^{2ix}) = \frac{1}{2} ie^{ix} - \frac{1}{2} ie^{-ix} = \frac{e^{-ix} - e^{ix}}{2i} = -g \]
    \[ g(0) = \frac{e^{i \cdot 0} - e^{-i \cdot 0}}{2i} = \frac{1 - 1}{2i} = 0 \]
    Hence, we verified that, 
    \[ sin(x) = \frac{e^{ix} - e^{-ix}}{2i} \]
    Similarly, for $h = cos(x)$, it has the unique solution to $h'' = -h$, $h(0) = 1$, $h'(0) = 0$. 
    We want to check whether the performance of $l = \frac{e^{ix} + e^{-ix}}{2}$ matches
    \[ l' = \frac{1}{2}ie^{-ix}(-1 + e^{2ix}) \Rightarrow l'(0) = -\frac{1}{2} + \frac{1}{2} = 0 \]
    \[ l'' = -\frac{1}{2} e^{-ix}(1 + e^{2ix}) = -\frac{1}{2} e^{-ix} - \frac{1}{2} e^{ix} = -\frac{e^{ix} + e^{-ix}}{2} = -l \]
    \[ l(0) = \frac{e^{ix} + e^{-ix}}{2} = \frac{1 + 1}{2} = 1 \]
    Hence, we also verified that, 
    \[ cos(x) = \frac{e^{ix} + e^{-ix}}{2} \]
\end{proof}

\subsubsection*{(b)}
\begin{proof}
    Using the result from part (a):
    \[
        \cos(x) = \frac{e^{ix} + e^{-ix}}{2}, \quad 
        i \sin(x) = i \left( \frac{e^{ix} - e^{-ix}}{2i} \right) = \frac{e^{ix} - e^{-ix}}{2}.
    \]
    Add \( \cos(x) \) and \( i \sin(x) \):
    \[
        \cos(x) + i \sin(x) = \frac{e^{ix} + e^{-ix}}{2} + \frac{e^{ix} - e^{-ix}}{2} = e^{ix}.
    \]
    Therefore:
    \[
        e^{ix} = \cos(x) + i \sin(x).
    \]
\end{proof}

\subsubsection*{(c)}
\begin{proof}
    Express \( \cos(x) + i \sin(x) \) using Euler's formula:
    \[
        \cos(x) + i \sin(x) = e^{ix}.
    \]

    Raise both sides to the \( n \)-th power:
    \[
        [\cos(x) + i \sin(x)]^n = (e^{ix})^n = e^{inx}.
    \]
    Express \( e^{inx} \) back in terms of cosine and sine:
    \[
        e^{inx} = \cos(nx) + i \sin(nx).
    \]
    Combine the results:
    \[
        (\cos(x) + i \sin(x))^n = \cos(nx) + i \sin(nx).
    \]
\end{proof}

\subsubsection*{(d) Application with $n = 2$ to show double angle identity}
\begin{proof}
    Use De Moivre's formula with \( n = 2 \):
    \[
        (\cos(x) + i \sin(x))^2 = \cos(2x) + i \sin(2x).
    \]
    Expand the left-hand side using algebra:
    \[
        (\cos(x) + i \sin(x))^2 = \cos^2(x) + 2i \cos(x) \sin(x) - \sin^2(x).
    \]
    Note that \( i^2 = -1 \). 
    \\
    Separate real and imaginary parts:
    \begin{itemize}
        \item Real part:
        \[
        \cos^2(x) - \sin^2(x) = \cos(2x).
        \]
        \item Imaginary part:
        \[
        2 \cos(x) \sin(x) = \sin(2x).
        \]
    \end{itemize}
\end{proof}

\begin{problem}
    Consider the system of differential equations:
    \begin{align*}
        X' = -5X + 9Y\\
        Y' = -4X + 7Y
    \end{align*}
    with initial condition \(X(0) = 1\) and \(Y(0) = 1\).
    
    Here's a Jordan decomposition
    \[\begin{bmatrix}
        -5&9\\
        -4&7
    \end{bmatrix}=
    \begin{bmatrix}
        3&1\\2&1
    \end{bmatrix}
    \begin{bmatrix}
        1&1\\0&1
    \end{bmatrix}
    \begin{bmatrix}
        3&1\\2&1
    \end{bmatrix}\inv\]
    where
    \[
    \begin{bmatrix}
        3&1\\2&1
    \end{bmatrix}\inv=
    \begin{bmatrix}
        1&-1\\-2&3
    \end{bmatrix}\inv\]

    \begin{enumerate}[a)]
        \item Solve this differential equation exactly.
        \item Use (a) to determine \(X(2)\) and \(Y(2)\).
        \item Estimate \(X(2)\) and \(Y(2)\) using Euler's method two ways: one step and two steps.
    \end{enumerate}
\end{problem}

\subsubsection*{(a)}
\begin{proof}
    To solve the system of differential equations:
    \[ X' = AX \]
    Where 
    \[ A = 
        \begin{bmatrix}
            -5 & 9 \\ -4 & 7
        \end{bmatrix},
        \quad
        X(0) = 
        \begin{bmatrix}
            1 \\ 1
        \end{bmatrix},
        \quad
    \]
    The general solution to the system is:
    \[ X(t) = Pe{Jt}P^{-1}X(0) \]
    Given teh Jordan block is:
    \[ J = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \] 
    The matrix exponential $e^{Jt}$ for a Jordan block is:
    \[ e^{Jt} = e^{\lambda t}(I + Nt) \]
    Where $\lambda = 1$ is the eigenvalue and $N = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$ is the nilpotent matrix. ThusL
    \[ e^{Jt} = e^t \begin{bmatrix} 1 & t \\ 0 & 1 \end{bmatrix} \]
    Refer back to the generao solution, first lets compute $P^{-1}X(0)$:
    \[ P^{-1}X(0) = \begin{bmatrix} 1 & -1 \\ -2 & 3 \end{bmatrix} \begin{bmatrix} 1  \\ 1 \end{bmatrix} = \begin{bmatrix} 1 - 1 \\ -2 + 3 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \]
    Then, we can get $e^{Jt}P^{-1}X(0)$:
    \[ e^{Jt}P^{-1}X(0) = e^t \begin{bmatrix} 1 & t \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = e^t \begin{bmatrix} t \\ 1 \end{bmatrix} \]
    Lastly, we left multiply $P$ to get $X(t)$:
    \[ X(t) = P \left( e^t \begin{bmatrix} t \\ 1 \end{bmatrix} \right) = e^t \begin{bmatrix} 3 & 1 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} t \\ 1 \end{bmatrix} = e^t \begin{bmatrix} 3t + 1 \\ 2t + 1 \end{bmatrix} \]
    Hence,
    \[ X(t) = e^t(1 + 3t), \quad Y(t) = e^t (1 + 2t) \]
\end{proof} 

\subsubsection*{(b)}
\begin{proof}
    Refer to the result in (a), when $t = 2$:
    \[ X(2) = e^2(1 + 3 \times 2) = e^2 \times 7 = 7e^2 \]
    \[ Y(2) = e^2(1 + 2 \times 2) = e^2 \times 5 = 5e^2 \]
\end{proof}

\subsubsection*{(c)}
Euler's method is a numerical approach to approximate the solution to a system of differential equations:
\[
    \mathbf{X}' = A \mathbf{X},
\]
with initial condition \( \mathbf{X}(t_0) = \mathbf{X}_0 \). The update formula is:
\[
    \mathbf{X}_{n+1} = \mathbf{X}_n + h A \mathbf{X}_n,
\]

\begin{proof}
    Given,
    \[ A = 
        \begin{bmatrix}
            -5 & 9 \\ -4 & 7
        \end{bmatrix},
        \quad
        X(0) = 
        \begin{bmatrix}
            1 \\ 1
        \end{bmatrix},
        \quad
    \]
    1. One step strategy (From $t = 0$ to $t = 2$, with $h = 2$):
    \\
    \[ AX_0 = \begin{bmatrix} -5 & 9 \\ -4 & 7 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 4 \\ 3 \end{bmatrix} \]
    By our update rule:
    \[ X_1 = X_0 + 2 \times \begin{bmatrix} 4 \\ 3 \end{bmatrix} = \begin{bmatrix} 1 + 8 \\ 1 + 6 \end{bmatrix} = \begin{bmatrix} 9 \\ 7 \end{bmatrix} \]
    So the estimation will be:
    \[ X(2) \approx 9, Y(2) \approx 7 \]
    2. Two steps strategy (From $t = 0$ to $t = 2$, with $h = 1$):
    \\
    From (1), we can get
    \[ AX_0 = \begin{bmatrix} 4 \\ 3 \end{bmatrix} \]
    By our update rulem this time, fisrt:
    \[ X_1 = X_0 + 1 \times \begin{bmatrix} 4 \\ 3 \end{bmatrix} = \begin{bmatrix} 5 \\ 4 \end{bmatrix} \]
    Then,
    \[ AX_1 = \begin{bmatrix} -5 & 9 \\ -4 & 7 \end{bmatrix} \begin{bmatrix} 5 \\ 4 \end{bmatrix} = \begin{bmatrix} 11 \\ 8\end{bmatrix} \]
    \[ X_2 = X_1 + 1 \times \begin{bmatrix} 11 \\ 8 \end{bmatrix} = \begin{bmatrix} 16 \\ 12 \end{bmatrix} \]
    So the estimation this time will be:
    \[ X(2) \approx 16, Y(2) \approx 12 \]
\end{proof}

\noindent \textbf{Exact Values:}
\[
    X(2) = 7e^2 \approx 51.7237, \quad Y(2) = 5e^2 \approx 36.9455.
\]
\textbf{Euler's Estimates:}
\begin{itemize}
    \item \textbf{One Step (\( h = 2 \)):} \( X(2) \approx 9, \quad Y(2) \approx 7 \)
    \item \textbf{Two Steps (\( h = 1 \)):} \( X(2) \approx 16, \quad Y(2) \approx 12 \)
\end{itemize}

\noindent \textbf{Observations:}
\\
Both Euler's estimates are significantly lower than the exact values. The reason on having such behavior is that
Euler's method is a first-order numerical method with local truncation errors proportional to the step size \( h \). 
Larger step sizes exacerbate these errors, especially for systems with rapidly growing solutions like this one.

\begin{problem}
[Bonus]
    Let \(A\) and \(B\) be \(n\times n\) matrices.
    \begin{enumerate}[a)]
        \item Prove that
        \[e^{A+B} = e^Ae^B\]
        when \(AB=BA\).
        \item Give an example where
        \[e^{A+B} \neq e^Ae^B.\]
    \end{enumerate}
\end{problem}

\subsubsection*{(a)}
\begin{proof}
    The matrix exponential $e^M$ of a square matrix $M$ is defined by the power series:
    \[ e^M = \sum_{k = 0}^{\infty} \frac{M^k}{k!} \]
    Given that $A$ and $B$ commute, we can manipulate the exponential of their sum.
    \[ e^{A + B} = \sum_{k = 0}^{\infty} \frac{(A + B)^k}{k!} \]
    Since $A$ and $B$ commute, we can apply the binomial thm to expand $(A + B)^k$:
    \[ (A + B)^k = \sum_{j = 0}^{k} \begin{pmatrix} k \\ j \end{pmatrix} A^j B^{k - j} \]
    Sub it back, we can get:
    \[ e^{A + B} = \sum_{k = 0}^{\infty} \frac{1}{k!} \sum_{j = 0}^{k} \begin{pmatrix} k \\ j \end{pmatrix} A^j B^{k - j} = \sum_{j = 0}^{\infty} \sum_{k = j}^{\infty} \frac{\begin{pmatrix} k \\ j \end{pmatrix}}{k!} A^j B^{k - j} \]
    Noting that,
    \[ \frac{\begin{pmatrix} k \\ j \end{pmatrix}}{k!} = \frac{1}{j!(k - j)!} \]
    So by rearranging,
    \[ e^{A} e^{B} = \left( \sum_{m = 0}^{\infty} \frac{A^m}{m!} \right) \left( \sum_{mn= 0}^{\infty} \frac{B^n}{n!} \right) = \sum_{j = 0}^{\infty} \sum_{k = j}^{\infty} \frac{\begin{pmatrix} k \\ j \end{pmatrix}}{k!} A^j B^{k - j} = e^{A + B} \]
\end{proof}

\subsubsection*{(b)}
\section*{(b) Provide a simpler example where \( e^{A+B} \neq e^{A} e^{B} \)}

Let \( A \) and \( B \) be the \( 2 \times 2 \) matrices:
\[
    A = \begin{bmatrix}
    0 & 1 \\ 0 & 0
    \end{bmatrix}, \quad
    B = \begin{bmatrix}
    0 & 0 \\ 1 & 0
    \end{bmatrix}.
\]
Verify \( AB \neq BA \)
\[
    AB = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}, \quad
    BA = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}.
\]
Since \( AB \neq BA \), \( A \) and \( B \) do not commute.
\\
Noting that both \( A \) and \( B \) are nilpotent (\( A^2 = 0 \), \( B^2 = 0 \)),
\[
    e^{A} = I + A = \begin{bmatrix}
    1 & 1 \\ 0 & 1
    \end{bmatrix}, \quad
    e^{B} = I + B = \begin{bmatrix}
    1 & 0 \\ 1 & 1
    \end{bmatrix}.
\]
Now, we compute \( e^{A} e^{B} \)
\[
    e^{A} e^{B} = \begin{bmatrix}
    1 & 1 \\ 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 0 \\ 1 & 1
    \end{bmatrix}
    = \begin{bmatrix}
    2 & 1 \\ 1 & 1
    \end{bmatrix}.
\]
Then, we compute \( e^{A+B} \)
\[
    A + B = \begin{bmatrix}
    0 & 1 \\ 1 & 0
    \end{bmatrix}, \quad
    (A+B)^2 = I.
\]
Using the series expansion of the exponential, by definition, we can get:
\[
    e^{A+B} = \cosh(1) \cdot I + \sinh(1) \cdot (A+B).
\]
Numerical approximation (\( \cosh(1) \approx 1.5431 \), \( \sinh(1) \approx 1.1752 \)):
\[
    e^{A+B} \approx \begin{bmatrix}
    1.5431 & 1.1752 \\ 1.1752 & 1.5431
    \end{bmatrix}.
\]
But, by comparison
\[
    e^{A} e^{B} = \begin{bmatrix}
    2 & 1 \\ 1 & 1
    \end{bmatrix} \neq
    e^{A+B} \approx \begin{bmatrix}
    1.5431 & 1.1752 \\ 1.1752 & 1.5431
    \end{bmatrix}.
\]
Which match our expectation.
\end{document}
