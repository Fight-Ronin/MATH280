\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage{environ}
\usepackage{tikz}

\newcommand{\inv}{^{-1}}

\newcounter{pnum}
\NewEnviron{problem}{
    \stepcounter{pnum}
    \begin{center}
        \fbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{\thepnum.} \BODY
        \end{minipage}}
    \end{center}

    \textbf{Solution. }
}

\title{\vspace{-4em}Homework 6 (root finding)}
\author{Hanzhang Yin}
\begin{document}

\maketitle

\begin{problem}
    One view of the secant method: it is a coarser Newton's method. We've seen that it has some of the speed of Newton's method. One might also hope that it enjoys similar convergence properties.\\[-0.5em]
    
    Adapt the convergence proof for Newton's method to show that the secant method also always converges under the following assumptions about the function \(f\) on the interval \([a,b]\):
    \begin{enumerate}[\hspace{2em} i)]
        \item \(f\) is twice continuously differentiable
        \item \(f' > 0\)
        \item \(f''> 0\)
        \item \(f\) has a root \(x\) in the interval
        \item the two initial guesses \(x_0,x_1\) are both to the right of the root.
    \end{enumerate}

    Hint: you will have to use convexity in a slightly more interesting way than in NM -- the graph of \(f\) does not lie above the secant line, but you can argue that the right (well, left!) piece still does.
\end{problem}

\begin{proof}
    The secant method iterates according to the formula:
    \[
        x_{n+1} = x_n - f(x_n) \cdot \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}.
    \]
    We proceed in steps to show the convergence.
    \\
    \textbf{Monotonicity and Boundedness}
    \\
    \textit{Claim: } The sequence \( \{x_n\} \) is strictly decreasing and bounded below by \( x^\ast \).
    \\
    We will prove the claim by induction
    \begin{itemize}
        \item \textbf{Base Case (\( n = 1 \)):} By assumption, \( x_0 > x^\ast \) and \( x_1 > x^\ast \). WLOG, we can reorder \( x_0 \) and \( x_1 \) such that \( x_0 > x_1 > x^\ast \). Hence, the base case holds.
    
        \item \textbf{Inductive Step:} Assume \( x_{n-2} > x_{n-1} > x^\ast \). We show that \( x_n > x^\ast \) and \( x_n < x_{n-1} \).
    
        From the secant update:
        \[
        x_n = x_{n-1} - f(x_{n-1}) \cdot \frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-2})}.
        \]
        \begin{itemize}
            \item Since \( x_{n-2} > x_{n-1} \), we have \( x_{n-1} - x_{n-2} < 0 \).
            \item Since \( f'(x) > 0 \) on \([a, b]\), \( f(x_{n-1}) > f(x_{n-2}) \), so \( f(x_{n-1}) - f(x_{n-2}) > 0 \).
            \item Therefore, the ratio \( \frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-2})} < 0 \).
            \item Since \( f(x_{n-1}) > 0 \) (as \( x_{n-1} > x^\ast \ \text{and f is increasing} \)), the term subtracted from \( x_{n-1} \) is positive:
            \[ x_n = x_{n-1} - (\text{Positive Number}) < x_{n-1} \]
            implying \( x_n < x_{n-1} \).
            \item To show \( x_n > x^\ast \), assume \( x_n \leq x^\ast \). Then \( f(x_n) \leq f(x^\ast) = 0 \), contradicting the fact that \( f(x_n) > 0 \) for \( x_n > x^\ast \). Thus, \( x_n > x^\ast \).
        \end{itemize}
    \end{itemize}
    By induction, \( \{x_n\} \) is strictly decreasing and bounded below by \( x^\ast \).
    \\
    \textbf{Convergence of the Sequence}
    \\
    \textit{Claim: } The sequence \( \{x_n\} \) converges to \( x^\ast \).
    \\
    Since \( \{x_n\} \) is strictly decreasing and bounded below by \( x^\ast \), it converges to some limit \( l \geq x^\ast \) by \textbf{MCT}. Suppose, for contradiction, that \( l > x^\ast \).
    
    \begin{itemize}
        \item Since \( f \) is continuous and strictly increasing:
        \[
        \lim_{n \to \infty} f(x_n) = f(l) > f(x^\ast) = 0.
        \]
        \item Consider the secant update:
        \[
        x_{n+1} = x_n - \frac{f(x_n)}{s_n}, \quad \text{where } s_n = \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}.
        \]
        \item Because \( f \) is convex (\( f'' > 0 \)), the slope \( s_n > f'(x^\ast) > 0 \), so:
        \[
        \left| \frac{f(x_n)}{s_n} \right| < \frac{f(x_n)}{f'(x^\ast)}.
        \]
        \item As \( n \to \infty \), \( f(x_n) \to f(l) > 0 \), meaning the step sizes \( x_n - x_{n+1} \) do not shrink to zero.
        \item This contradicts the convergence \( x_n \to l \), as the step sizes must tend to zero for convergence.
    \end{itemize}
    Thus, \( l = x^\ast \), and the sequence converges to \( x^\ast \).
    \\
    The convexity of \( f \) ensures that the secant line between any two points lies below the graph of \( f \), preventing the iterates \( x_n \) from overshooting the root \( x^\ast \). Thus, \( x_n > x^\ast \) for all \( n \).
    Under the given assumptions:
    \begin{enumerate}
        \item \( \{x_n\} \) is strictly decreasing and bounded below by \( x^\ast \),
        \item By the monotone convergence theorem, \( \{x_n\} \) converges to a limit \( l \geq x^\ast \),
        \item Assuming \( l > x^\ast \) leads to a contradiction, hence \( l = x^\ast \),
        \item Convexity ensures no overshooting, maintaining \( x_n > x^\ast \).
    \end{enumerate}
    \\
    Therefore, the secant method converges to the root \( x^\ast \).
\end{proof}

\newpage

\begin{problem}
    Another view of the secant method, discussed in class, is as a weighted bisection method. Here too, one might hope for a convergence guarantee, because BM is much more robust than NM in that regard.\\[-0.5em]

    Consider a modified secant method which at step \(k\) takes in endpoints \(a_k,b_k\), calculates their weighted midpoint \(c_k\) and then returns two new endpoints \(a_{k+1},b_{k+1}\), one of which is \(c_k\), to which IVT applies. These new endpoints are input to the next step.\\[-0.5em]

    Prove that if \(f\) is continuous on \([a,b] = [a_0,b_0]\) and the IVT applies to \(f\) on the interval, then the sequence \(c_k\) from the modified secant method converges to a root of \(f\).\\[-0.5em]

    Hint: the reason for convergence is \emph{not} the same as for bisection. This would require the stronger assumption that \(f\) is continuously differentiable. In fact:

    [Bonus] Give an example where the sequences \(x_k\) and \(y_k\) converge to different points, so squeeze does not apply.
\end{problem}

\begin{proof}
From Bisection Method, at each iteration \( k \), we have: 
\\
\begin{enumerate}
    \item \( f(a_k) \cdot f(b_k) < 0 \), so \( f \) changes sign on \( [a_k, b_k] \).
    \item The point \( c_k \) is computed using the secant method formula:
    \[
        c_k = b_k - f(b_k) \cdot \frac{b_k - a_k}{f(b_k) - f(a_k)}.
    \]
    \item One of the endpoints \( a_{k+1} \) or \( b_{k+1} \) is set to \( c_k \), and the other remains \( a_k \) or \( b_k \), such that \( f(a_{k+1}) \cdot f(b_{k+1}) < 0 \).
    \item *The intervals \( [a_k, b_k] \) may not necessarily shrink, but they always contain a root of \( f \) due to the IVT.
\end{enumerate}
\\
Since \( \{ c_k \} \) is bounded in \( [a_0, b_0] \), by the Bolzano-Weierstrass Theorem, there exists a subsequence \( \{ c_{k_j} \} \) that converges to some limit \( c^\ast \in [a_0, b_0] \).
Our goal is to show that \( f(c^\ast) = 0 \).
\\
Suppose, for contradiction, that \( f(c^\ast) \neq 0 \).
\\
\textit{Case 1:} \( f(c^\ast) > 0 \):
\\
Since \( f \) is continuous, there exists \( \delta > 0 \) such that \( f(x) > 0 \) for all \( x \in (c^\ast - \delta, c^\ast + \delta) \).
\\
Because \( c_{k_j} \to c^\ast \), for sufficiently large \( j \), \( c_{k_j} \in (c^\ast - \delta, c^\ast + \delta) \), and \( f(c_{k_j}) > 0 \).
\\
Similarly, since \( f(a_{k_j}) \cdot f(b_{k_j}) < 0 \), and \( c_{k_j} \) replaces either \( a_{k_j} \) or \( b_{k_j} \), we have:
\begin{itemize}
    \item If \( f(c_{k_j}) \cdot f(a_{k_j}) < 0 \), then \( f(a_{k_j}) < 0 \).
    \item If \( f(c_{k_j}) \cdot f(b_{k_j}) < 0 \), then \( f(b_{k_j}) < 0 \).
\end{itemize}
\\
Thus, there is a sequence of points \( \{ d_{k_j} \} \subset [a_0, b_0] \) (either \( a_{k_j} \) or \( b_{k_j} \)) such that \( f(d_{k_j}) < 0 \).
\\
Since \( \{ d_{k_j} \} \) is bounded in \( [a_0, b_0] \), it has a convergent subsequence \( \{ d_{k_{j_m}} \} \) converging to some \( d^\ast \in [a_0, b_0] \).
\\
Because \( f \) is continuous, \( f(d^\ast) = \lim_{m \to \infty} f(d_{k_{j_m}}) \leq 0 \). In fact, \( f(d^\ast) < 0 \), since \( f(d_{k_{j_m}}) < 0 \) for all \( m \).
\\
But since \( c_{k_{j_m}} \to c^\ast \) and \( d_{k_{j_m}} \to d^\ast \), and \( f(c^\ast) > 0 \) and \( f(d^\ast) < 0 \), we have \( f(c^\ast) \cdot f(d^\ast) < 0 \).
\\
Therefore, by the Intermediate Value Theorem, there exists a ``new'' point \( x^\ast \) (i.e. a root) between \( c^\ast \) and \( d^\ast \) such that \( f(x^\ast) = 0 \), contradicting the assumption.
\\
\textit{Case 2:} \( f(c^\ast) < 0 \):
\\
A similar argument leads to a contradiction as well in this case, showing that must \( f(c^\ast) = 0 \).
\\
We have shown that any convergent subsequence of \( \{ c_k \} \) converges to a root \( c^\ast \) of \( f \). To show that \( \{ c_k \} \) itself converges to \( c^\ast \), suppose, for contradiction, that \( \{ c_k \} \) has another subsequence \( \{ c_{k_i} \} \) converging to a different limit \( c^\dagger \neq c^\ast \).
Using the same argument as in Step 3, we can show that \( f(c^\dagger) = 0 \).
\\
Since \( f \) is continuous and the initial interval \( [a_0, b_0] \) contains only one root (due to the IVT and the sign change), it must be that \( c^\dagger = c^\ast \). Therefore, all convergent subsequences of \( \{ c_k \} \) converge to \( c^\ast \). Thus, the sequence \( \{ c_k \} \) converges to \( c^\ast \).

\end{proof}


\begin{problem}
    Suppose \(f(x)\) and \(g(x)\) are functions with a common root \(x=a\).
    \begin{enumerate}[\hspace{2em}a)]
        \item Prove that a solution to the homotopy continuation initial value problem
    \[x'(t) = -\frac{H_t}{H_x}\ \ \ \ x(0) = a\]
    is the constant function \(x=a\).
        \item Give an example where the solution above is \emph{not} unique.
    \end{enumerate}

    Hint: see handout for a picture of (a). Think about how it could be adapted (b); you can even use the tool to help you construct an example.
\end{problem}

\begin{proof}
    \textbf{Part (a): Proving \( x(t) = a \) is a Solution}
    \\
    Let us define the homotopy \( H(x, t) \) as
    \[
        H(x, t) = (1 - t) f(x) + t g(x).
    \]
    Since \( f(a) = g(a) = 0 \), it follows that \( H(a, t) = 0 \) for all \( t \in [0, 1] \).
    \\
    We need to show that \( x(t) = a \) satisfies the differential equation
    \[
        x'(t) = -\frac{H_t}{H_x}, \quad x(0) = a.
    \]
    \textit{Computing the Partial Derivatives:}
    \\
    First, compute \( H_t \) and \( H_x \):
    \[
        H_t(x, t) = -f(x) + g(x),
    \]
    \[
        H_x(x, t) = (1 - t) f'(x) + t g'(x).
    \]
    Evaluate these at \( x = a \):
    \[
        H_t(a, t) = -f(a) + g(a) = -0 + 0 = 0.
    \]
    \[
        H_x(a, t) = (1 - t) f'(a) + t g'(a).
    \]
    Note that \( H_x(a, t) \) may not be zero unless both \( f'(a) \) and \( g'(a) \) are zero.
    \\
    \textit{Computing \( x'(t) \) at \( x = a \):}
    \\
    Substitute \( x(t) = a \) into the differential equation:
    \[
        x'(t) = -\frac{H_t(a, t)}{H_x(a, t)} = -\frac{0}{H_x(a, t)} = 0.
    \]
    Therefore,
    \[
        x'(t) = 0, \quad x(0) = a.
    \]
    This implies that \( x(t) = a \) for all \( t \in [0, 1] \).
    \\
    \textbf{Conclusion:}
    \\
    The constant function \( x(t) = a \) is a solution to the homotopy continuation initial value problem.
\end{proof}

\begin{proof}
    \textbf{Part (b): Example Where the Solution is Not Unique}
    \\
    We will construct specific functions \( f(x) \) and \( g(x) \) with a common root at \( x = a \) such that the initial value problem
    \[
        x'(t) = -\frac{H_t}{H_x}, \quad x(0) = a,
    \]
    has multiple solutions.
    \\
    \textbf{Example Functions:}
    \\
    Let
    \[
        f(x) = (x - a)^{1/3}, \quad g(x) = -(x - a)^{1/3}.
    \]
    Both functions have a root at \( x = a \):
    \[
        f(a) = g(a) = 0.
    \]
    \textbf{Constructing the Homotopy:}
    \\
    Define
    \[
        H(x, t) = (1 - t) f(x) + t g(x) = (1 - t)(x - a)^{1/3} + t ( - (x - a)^{1/3} ) = (1 - 2t)(x - a)^{1/3}.
    \]
    Compute \( H_t \) and \( H_x \):
    \[
        H_t(x, t) = -f(x) + g(x) = - (x - a)^{1/3} - (x - a)^{1/3} = -2 (x - a)^{1/3},
    \]
    \[
        H_x(x, t) = (1 - 2t) \cdot \frac{1}{3} (x - a)^{-2/3}.
    \]

    \begin{tikzpicture}
        \begin{scope}[xshift=6cm] % Shift this graph horizontally
            % Box and divisions
            \draw[thick] (0,0) rectangle (2,2);
            \draw[thick] (1,0) -- (1,2); % Vertical line
            \draw[thick] (0,1) -- (2,1); % Horizontal line
        
            % Labels
            \node[below] at (2,0) {0};
            \node[left] at (0,1) {\(x= \frac{1}{2}\)};
            \node[right] at (2,2) {1};
            \node[right] at (2,1) {\(x=1\)};
            \node[below] at (1,0) {\(a\)};
        \end{scope}
    \end{tikzpicture}
    \\
    When $t$ shifts from $0 \to 1$, at a certain point for $t$ the homotopy will coincide with the x-axis completely. 
    This leads a shift from the solution $x=a$ when $t=0$ to the solution $x$ equals to every possible points on the x-axis between $x = \frac{1}{2}$ to $x = 1$.
    This clearly makes the solution \textbf{NOT Unique} during the approximation.

\end{proof}

\end{document}
